{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junting-huang/data_storytelling/blob/main/case_2_number.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2rhhm5ieLbU"
      },
      "source": [
        "# case_2. number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we explored three widely used Python libraries: pandas, matplotlib, and NLTK. By combining these three packages, you can efficiently manipulate, visualize, and analyze data, including text data, in a variety of ways.\n",
        "\n",
        "* Pandas: Pandas is a robust data manipulation and analysis library, offering convenient data structures like DataFrame and Series for efficient handling and analysis of structured data. Pandas facilitates data manipulation tasks such as filtering, grouping, merging, and reshaping. It also handles missing data and supports dataset cleaning.\n",
        "\n",
        "* Matplotlib: Matplotlib stands out as a versatile 2D plotting library, enabling the creation of static, animated, and interactive visualizations in Python. Matplotlib supports diverse plot types like line plots, scatter plots, bar plots, and histograms.\n",
        "\n",
        "* NLTK (Natural Language Toolkit): NLTK serves as a powerful library for natural language processing tasks, offering user-friendly interfaces for tasks like tokenization, stemming, tagging, parsing, and more. NLTK also provides functionality for named entity recognition, sentiment analysis, and other language-related tasks.\n",
        "\n",
        "\n",
        "Exploratory Data Analysis (EDA) on text data is a vital process that involves delving into the structure, characteristics, and content of textual information to uncover meaningful insights. Starting with an overview of the corpus, EDA includes essential tasks such as tokenization, preprocessing, and statistical analysis. Text statistics, visualizations like word clouds and bar charts, and N-gram analysis offer a comprehensive understanding of word frequencies, document lengths, and patterns beyond individual words. Sentiment analysis, named entity recognition (NER), topic modeling, and document similarity measures further enhance the exploration process. The use of interactive tools and consideration of contextual analysis contribute to a holistic approach in extracting valuable information from text data. \n",
        "\n",
        "\n",
        "The objective of this lab session is to guide you through the process of conducting Exploratory Data Analysis (EDA) on textual data. The focus is on providing practical insights into effectively exploring and understanding the characteristics of text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AExcvQC3eaAI"
      },
      "source": [
        "## 2.1 importing library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JwUxdOKeK3Z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmao288TeirV"
      },
      "source": [
        "## 2.2 importing corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8Rr9B_AfNpw"
      },
      "outputs": [],
      "source": [
        "# Reading the literary work from a text file\n",
        "filename = './data/walden.txt'\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK_R_DZifA7Y"
      },
      "source": [
        "## 2.3 tokenizing the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenization is a fundamental step in natural language processing (NLP) that involves breaking down a text into individual units, or tokens. We use Punkt tokenizer to tokeize text. The Punkt tokenizer is a pre-trained unsupervised machine learning model for tokenizing text into sentences. It is part of the Natural Language Toolkit (NLTK) library. The Punkt tokenizer uses a combination of unsupervised and rule-based methods to achieve accurate sentence segmentation. It is trained on a large corpus of text and is capable of handling various languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa_OWPCUesrs",
        "outputId": "fb58eca6-92b0-4190-a0ac-b6da4e1c2225"
      },
      "outputs": [],
      "source": [
        "# Downloading the Punkt Tokenizer Models\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Tokenizing the text\n",
        "tokens = word_tokenize(text.lower())  # Convert text to lowercase and tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The .isalnum() method is a string method in Python that checks whether all the characters in a given string are alphanumeric. Alphanumeric characters are those that are either alphabets (a-z or A-Z) or numeric digits (0-9). If all the characters in the string are alphanumeric, the method returns True; otherwise, it returns False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnPthJ2ke32y"
      },
      "outputs": [],
      "source": [
        "# Removing stopwords and punctuation\n",
        "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
        "\n",
        "# Counting word frequencies\n",
        "word_frequencies = nltk.FreqDist(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(word_frequencies.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBujswQOfgQ1"
      },
      "source": [
        "## 2.4 counting words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a DataFrame named df from a dictionary of word frequencies with columns 'Word' and 'Frequency'. After that, we sort the DataFrame based on the 'Frequency' column in descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ydZyA0Ze-xQ"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame\n",
        "df = pd.DataFrame(word_frequencies.items(), columns=['Word', 'Frequency'])\n",
        "\n",
        "# Sorting the DataFrame by Frequency\n",
        "df = df.sort_values(by='Frequency', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Showing the top 5 frequent words\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO1YYhnEf4B7"
      },
      "source": [
        "## 2.5 plotting the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a data analysis task, we often want to use visualizations to gain insight on the data/text at hand. For example, creating a plot for the top-k frequent words, often in the form of a bar chart or a word cloud, serves several purposes in data analysis and text visualization:\n",
        "\n",
        "* Identify Key Themes: Analyzing the most frequent words helps identify key themes, topics, or subjects present in the text. This is particularly useful for understanding the primary focus or content of a document or corpus.\n",
        "\n",
        "* Content Summary: The top words provide a concise summary of the content. This is beneficial when dealing with large amounts of text, allowing users to quickly grasp the main ideas without reading the entire text.\n",
        "\n",
        "* Insights into Language Usage: Analyzing frequent words offers insights into language usage and style. It helps understand the vocabulary and common phrases used, providing context about the writing style.\n",
        "\n",
        "We use matplotlib package to create a bar plot of the top 20 frequent words from our DataFrame df. The code below specifies the figure size, uses the head(20) method to select the top 20 frequent words, and then creates a bar plot using the 'Word' column for the x-axis and the 'Frequency' column for the y-axis. If you're looking for more information on plotting with Matplotlib, please check out: https://matplotlib.org/stable/users/explain/quick_start.html#quick-start. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "WINTtTECf1-r",
        "outputId": "a515db8a-9e5a-473f-8935-9be6407599ef"
      },
      "outputs": [],
      "source": [
        "# Plotting the top 20 frequent words\n",
        "plt.figure(figsize=(20,6))\n",
        "df.head(20).plot(x='Word', y='Frequency', kind='bar', legend=False, color='teal')\n",
        "plt.title('Top 10 Most Frequent Words')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is this word frequency bar plot useful in understanding the key themes or topics within the Walden?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPHit5ppks0O"
      },
      "source": [
        "## 2.5 term frequency - inverse document frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects the importance of a term in a document relative to a collection of documents (corpus). It is commonly used in natural language processing and information retrieval to extract meaningful information from a large set of documents. TF-IDF consists of two main components:\n",
        "\n",
        "Term Frequency (TF): Measures how often a term appears in a document. It is calculated as the ratio of the number of times a term occurs in a document to the total number of terms in that document. The idea is to give higher weights to terms that appear frequently in a document, as they are likely to be more significant.\n",
        "​\n",
        "\n",
        "Inverse Document Frequency (IDF): Measures how important a term is across the entire corpus. It is calculated as the logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the term.\n",
        "The goal is to assign higher weights to terms that are rare across the entire corpus, making them more distinctive.\n",
        "\n",
        "\n",
        "By multiplying these values together we can get our final TF-IDF value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we use the TfidfVectorizer from sklearn package to convert Walden text into a TF-IDF matrix. The resulting tfidf_matrix is a sparse matrix where each row corresponds to a sentence, and each column corresponds to a unique term in the vocabulary. The values in the matrix represent the TF-IDF scores for each term in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t2wfzLHkGEx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a DataFrame named df from a dictionary of word importance with index by 'Word'. After that, we sort the DataFrame based on the 'Importance' in descending order. Similarly, we use matplotlib package to create a bar plot of the top 20 important words from our DataFrame df. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VfpJXMokQBi"
      },
      "outputs": [],
      "source": [
        "# Getting feature names (words) from the vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Summing the TF-IDF scores for each word\n",
        "word_importance = tfidf_matrix.sum(axis=0)\n",
        "\n",
        "# Creating a DataFrame to store words and their importance\n",
        "df = pd.DataFrame(word_importance.T, index=feature_names, columns=[\"Importance\"])\n",
        "\n",
        "# Sorting the DataFrame by Importance\n",
        "df = df.sort_values(by='Importance', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "YMkXNQUSkXmE",
        "outputId": "d111e7c0-80ad-4c48-f481-8e29baf3fbdb"
      },
      "outputs": [],
      "source": [
        "# Plotting the top 20 most important words\n",
        "plt.figure(figsize=(20,6))\n",
        "df.head(20).plot(kind='bar', legend=False, color='teal', figsize=(10,6))\n",
        "plt.title('Top 20 Most Important Words')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is this word frequency bar plot more informative than the previous one?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Word Cloud\n",
        "\n",
        "Word Cloud is a another data visualization technique used to represent the most frequently occurring words in a given text or corpus. It provides a visually striking and intuitive way to analyze and understand the prominent words in a body of text. The size of each word in the cloud is proportional to its frequency or importance in the text.\n",
        "\n",
        "In python, you can use *WordCloud* library to create a word cloud. *WordCloud* is widely used in data analysis, text mining, and exploratory data analysis to quickly grasp the most relevant terms in a given textual dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the generated word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMDWa8QRdA9B8IYfc75976D",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
