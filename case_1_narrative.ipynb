{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junting-huang/data_storytelling/blob/main/case_1_narrative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUrLBPV3Qdpt"
      },
      "source": [
        "# case_1. narrative\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. More information can be found: https://textblob.readthedocs.io/en/dev/. This tutorial serves as a quick introduction to the python TextBlob package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPkq74HLWLvm"
      },
      "source": [
        "## 1.1 installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tqDxERkWnEC",
        "outputId": "61aa3161-19b7-4c7a-d401-33d54ad2f674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m104.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /Users/liyao/anaconda3/envs/py38/lib/python3.8/site-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /Users/liyao/anaconda3/envs/py38/lib/python3.8/site-packages (from nltk>=3.1->textblob) (8.1.6)\n",
            "Requirement already satisfied: joblib in /Users/liyao/anaconda3/envs/py38/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/liyao/anaconda3/envs/py38/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /Users/liyao/anaconda3/envs/py38/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.17.1\n"
          ]
        }
      ],
      "source": [
        "! pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd1UgyKtbbhc",
        "outputId": "76afdddf-7885-4495-f572-fe46e449e1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /Users/liyao/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /Users/liyao/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/liyao/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/liyao/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /Users/liyao/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/liyao/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "! python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwUyLnQYV3qI"
      },
      "source": [
        "## 1.2 basic usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat TextBlob objects as if they were Python strings that learned how to do Natural Language Processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, the import."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FhMDu1yhXNzc"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s create our first TextBlob."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YK1Mt16rX4Hg"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(\"When I wrote the following pages, or rather the bulk of them, I lived alone, in the woods, a mile from any neighbor, in a house which I had built myself, on the shore of Walden Pond, in Concord, Massachusetts, and earned my living by the labor of my hands only. \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part-of-speech Tagging\n",
        "\n",
        "Part-of-speech tags can be accessed through the tags property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtCAIT5GX-5U",
        "outputId": "ec76fb9f-4fc0-4aaf-a5ee-827b54faafa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('When', 'WRB'), ('I', 'PRP'), ('wrote', 'VBD'), ('the', 'DT'), ('following', 'JJ'), ('pages', 'NNS'), ('or', 'CC'), ('rather', 'RB'), ('the', 'DT'), ('bulk', 'NN'), ('of', 'IN'), ('them', 'PRP'), ('I', 'PRP'), ('lived', 'VBD'), ('alone', 'RB'), ('in', 'IN'), ('the', 'DT'), ('woods', 'NNS'), ('a', 'DT'), ('mile', 'NN'), ('from', 'IN'), ('any', 'DT'), ('neighbor', 'NN'), ('in', 'IN'), ('a', 'DT'), ('house', 'NN'), ('which', 'WDT'), ('I', 'PRP'), ('had', 'VBD'), ('built', 'VBN'), ('myself', 'PRP'), ('on', 'IN'), ('the', 'DT'), ('shore', 'NN'), ('of', 'IN'), ('Walden', 'NNP'), ('Pond', 'NNP'), ('in', 'IN'), ('Concord', 'NNP'), ('Massachusetts', 'NNP'), ('and', 'CC'), ('earned', 'VBD'), ('my', 'PRP$'), ('living', 'NN'), ('by', 'IN'), ('the', 'DT'), ('labor', 'NN'), ('of', 'IN'), ('my', 'PRP$'), ('hands', 'NNS'), ('only', 'RB')]\n"
          ]
        }
      ],
      "source": [
        "print(blob.tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Noun Phrase Extraction\n",
        "\n",
        "Similarly, noun phrases are accessed through the noun_phrases property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pCvZ6vzYCk1",
        "outputId": "4dcbc315-3e47-43d4-fd30-26348910ed02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['walden pond', 'concord', 'massachusetts']\n"
          ]
        }
      ],
      "source": [
        "print(blob.noun_phrases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization\n",
        "\n",
        "You can break TextBlobs into words or sentences. Sentence objects have the same properties and methods as TextBlobs. For more advanced tokenization, see the Advanced Usage guide: https://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "WordList(['When', 'I', 'wrote', 'the', 'following', 'pages', 'or', 'rather', 'the', 'bulk', 'of', 'them', 'I', 'lived', 'alone', 'in', 'the', 'woods', 'a', 'mile', 'from', 'any', 'neighbor', 'in', 'a', 'house', 'which', 'I', 'had', 'built', 'myself', 'on', 'the', 'shore', 'of', 'Walden', 'Pond', 'in', 'Concord', 'Massachusetts', 'and', 'earned', 'my', 'living', 'by', 'the', 'labor', 'of', 'my', 'hands', 'only'])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "blob.words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Sentence(\"When I wrote the following pages, or rather the bulk of them, I lived alone, in the woods, a mile from any neighbor, in a house which I had built myself, on the shore of Walden Pond, in Concord, Massachusetts, and earned my living by the labor of my hands only.\")]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "blob.sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Words Inflection and Lemmatization\n",
        "\n",
        "Each word in TextBlob.words or Sentence.words is a Word object (a subclass of unicode) with useful methods, e.g. for word inflection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'pages'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(blob.words[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'page'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(blob.words[5].singularize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mile'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(blob.words[19])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "miles\n"
          ]
        }
      ],
      "source": [
        "print(blob.words[19].pluralize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Words can be lemmatized by calling the lemmatize method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote\n"
          ]
        }
      ],
      "source": [
        "print(blob.words[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QWWCCw1YHLq",
        "outputId": "50464016-f3b7-433a-d8af-442735a11321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "write\n"
          ]
        }
      ],
      "source": [
        "print(blob.words[2].lemmatize('v'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "following\n"
          ]
        }
      ],
      "source": [
        "print(blob.words[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_i9HM9vadpE",
        "outputId": "ec6939fe-d9c0-49e9-9da3-4d8213923c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "follow\n"
          ]
        }
      ],
      "source": [
        "print(blob.words[4].stem())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A complete tutorial from TextBlob official website can be found here: https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
