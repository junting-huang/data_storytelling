{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case_3. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last lab, we introduced exploratory data analysis (EDA) on textual data using popular python packages such as pandas, matplotlib, and NLTK. However, real-world textual data often comes in messy and unstructured formats, presenting challenges for analysis and interpretation. This lab serves as an introductory guide, demonstrating how to preprocess raw textual data obtained from the internet, scraped from websites, or sourced from other data repositories.\n",
    "\n",
    "Text preprocessing is an essential step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for analysis. It includes tokenization, stemming, lemmatization, stop-word and punctuation removal, and part-of-speech tagging. The goal is to enhance the quality of the textual data, reduce noise, and standardize its structure. In this lab, we will introduce the basics of text preprocessing and provide Python code examples to illustrate how to implement these tasks using the NLTK library. By the end of the lab, readers will better understand how to prepare text data for many NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import packages and dataset\n",
    "\n",
    "As always, we need to import packages and raw dataset. In Python's scikit-learn library, the *sklearn.datasets* module provides a collection of functions and utilities for loading and fetching standard datasets for machine learning. This module is particularly useful for quickly experimenting with algorithms and performing initial data exploration. It includes functions to download and load various well-known datasets, such as the iris dataset, digits dataset, and breast cancer dataset, among others. These datasets cover a range of problem types, including classification, regression, and clustering. The sklearn.datasets module simplifies the process of acquiring sample datasets, making it convenient for practitioners to test and prototype machine learning models without the need for external data sources. This functionality is beneficial for both beginners learning machine learning concepts and experienced practitioners who want to quickly assess the performance of algorithms on standard datasets.\n",
    "\n",
    "\n",
    "The 20newsgroups dataset in Python, commonly used in the field of text classification and information retrieval. This dataset consists of approximately 20,000 newsgroup documents, covering 20 different newsgroups or topics. These topics span a diverse range of subjects, including politics, sports, technology, and more. The 20newsgroups dataset serves as a benchmark for text classification algorithms, allowing researchers and practitioners to evaluate the performance of models in distinguishing between various news categories. Each document in the dataset is labeled with its corresponding newsgroup, making it suitable for supervised learning tasks. The dataset is often utilized to test and develop algorithms for document classification, topic modeling, and text clustering, making it a valuable resource for the evaluation of natural language processing and machine learning techniques. In this lab, we use 20newsgroups dataset to demonstrate common text preprocessing tasks. Please feel free to replace this corpus to any text of your interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "\n",
    "documents, _ = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look of the first document in this corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = documents[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Stopwords Removal\n",
    "\n",
    "In NLP, stopwords refer to common words that are often removed from text data during preprocessing because they are considered to carry little meaningful information. These words typically include common grammatical terms, such as articles (e.g., \"the,\" \"a,\" \"an\"), prepositions (e.g., \"in,\" \"on,\" \"under\"), conjunctions (e.g., \"and,\" \"but,\" \"or\"), and other frequently occurring words. The purpose of removing stopwords is to focus on the more significant and distinctive words in a document, improving the efficiency of text analysis and reducing noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(list(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Filter the stopwords\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the preprocessed text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well 'm sure story nad seem biased . disagree statement U.S. Media ruin Israels reputation . rediculous . U.S. media pro-israeli media world . lived Europe realize incidences one described letter occured . U.S. media whole seem try ignore . U.S. subsidizing Israels existance Europeans ( least degree ) . think might reason report clearly atrocities . shame Austria , daily reports inhuman acts commited Israeli soldiers blessing received Government makes Holocaust guilt go away . , look Jews treating races got power . unfortunate .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Punctuation Removal\n",
    "\n",
    "Punctuation removal is a preprocessing step that involves eliminating punctuation marks from text data. Punctuation, such as periods, commas, exclamation points, and question marks, often doesn't contribute directly to the meaning of words and can introduce noise during text analysis.This process can be implemented using string manipulation techniques or regular expressions, where each character is examined, and punctuation marks are selectively excluded. The resulting text without punctuation is cleaner, making it more suitable for subsequent language processing tasks that rely on the semantic content of the text rather than its grammatical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuation using string manipulation\n",
    "text_no_punct = [char for char in filtered_words if char not in string.punctuation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the preprocessed text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well 'm sure story nad seem biased disagree statement U.S. Media ruin Israels reputation rediculous U.S. media pro-israeli media world lived Europe realize incidences one described letter occured U.S. media whole seem try ignore U.S. subsidizing Israels existance Europeans least degree think might reason report clearly atrocities shame Austria daily reports inhuman acts commited Israeli soldiers blessing received Government makes Holocaust guilt go away look Jews treating races got power unfortunate\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(text_no_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 lemmatization\n",
    "\n",
    "Lemmatization is an another fundamental technique that involves reducing words to their base or root forms, known as lemmas. Unlike stemming, which simply removes prefixes or suffixes to approximate the root form, lemmatization ensures that the transformed words are valid in the language. The process considers the context and grammatical structure of words, producing linguistically accurate lemmas. For instance, the lemmatization of words like \"running\" or \"ran\" would be \"run.\" This normalization step is crucial in NLP tasks where maintaining the semantic integrity of words is essential, such as in information retrieval, text mining, and sentiment analysis. Lemmatization helps unify variations of words, reducing dimensionality and improving the accuracy of language processing algorithms by focusing on the essential meaning carried by words in their canonical forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', \"' m\", 'sure', 'story', '', 'seem', 'bias', 'disagree', 'statement', '', 'medium', '', 'israel', 'reputation', 'rediculous', '', 'medium', 'pro - israeli', 'medium', 'world', 'live', '', 'realize', 'incidence', '', 'describe', 'letter', 'occur', '', 'medium', 'whole', 'seem', 'try', '', '', 'subsidize', 'israel', 'existance', 'european', 'least', 'degree', 'think', '', 'reason', 'report', 'clearly', 'atrocity', 'shame', '', 'daily', 'report', 'inhuman', 'act', 'commit', 'israeli', 'soldier', 'blessing', 'receive', 'government', 'make', '', 'guilt', 'go', 'away', 'look', '', 'treat', 'race', 'get', 'power', 'unfortunate']\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', \"' m\", 'sure', 'story', '', 'seem', 'bias', 'disagree', 'statement', '', 'medium', '', 'israel', 'reputation', 'rediculous', '', 'medium', 'pro - israeli', 'medium', 'world', 'live', '', 'realize', 'incidence', '', 'describe', 'letter', 'occur', '', 'medium', 'whole', 'seem', 'try', '', '', 'subsidize', 'israel', 'existance', 'european', 'least', 'degree', 'think', '', 'reason', 'report', 'clearly', 'atrocity', 'shame', '', 'daily', 'report', 'inhuman', 'act', 'commit', 'israeli', 'soldier', 'blessing', 'receive', 'government', 'make', '', 'guilt', 'go', 'away', 'look', '', 'treat', 'race', 'get', 'power', 'unfortunate']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_text = lemmatization(text_no_punct)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 gensim.utils.simple_preprocess function\n",
    "\n",
    "The *gensim.utils.simple_preprocess* function is part of the Gensim library, specifically in the gensim.utils module. It is a utility function designed for simple text preprocessing. The main purpose of this function is to tokenize and preprocess a text by performing the following operations:\n",
    "\n",
    "* Tokenization: It breaks down the input text into individual words or tokens.\n",
    "\n",
    "* Lowercasing: It converts all tokens to lowercase. This helps in standardizing the text and avoids treating the same word in different cases as different entities.\n",
    "\n",
    "* Removing Accent Marks (deacc=True): By default, the function removes accent marks from characters. This is useful for normalizing text, especially in scenarios where accent marks might not be relevant to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Note that the input text cannot be a list type\n",
    "processed_text = gensim.utils.simple_preprocess(\" \".join(lemmatized_text), deacc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'story', 'seem', 'bias', 'disagree', 'statement', 'medium', 'israel', 'reputation', 'rediculous', 'medium', 'pro', 'israeli', 'medium', 'world', 'live', 'realize', 'incidence', 'describe', 'letter', 'occur', 'medium', 'whole', 'seem', 'try', 'subsidize', 'israel', 'existance', 'european', 'least', 'degree', 'think', 'reason', 'report', 'clearly', 'atrocity', 'shame', 'daily', 'report', 'inhuman', 'act', 'commit', 'israeli', 'soldier', 'blessing', 'receive', 'government', 'make', 'guilt', 'go', 'away', 'look', 'treat', 'race', 'get', 'power', 'unfortunate']\n"
     ]
    }
   ],
   "source": [
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Topic Modeling\n",
    "\n",
    "\n",
    "Imagine you have a massive collection of documents, like articles, blog posts, or research papers. The challenge is to understand what these documents are about without reading each one individually. This is where topic modeling comes in. In the context of text, a topic is a set of words that tend to occur together. For example, if you're reading about cars, words like \"engine,\" \"speed,\" and \"fuel efficiency\" might frequently appear together, indicating the topic is related to automobiles. Topic modeling is a technique in the field of natural language processing (NLP) that helps us automatically discover hidden themes or topics in a large collection of text documents. Instead of reading every document, the idea is to let the computer analyze the words and find patterns, grouping them into topics.\n",
    "\n",
    "Topic models find useful applications in literature study by providing sophisticated tools to analyze, categorize, and derive insights from vast collections of texts. One significant application is in literature reviews, where researchers can employ topic modeling to efficiently identify and synthesize key themes across numerous academic papers, streamlining the process of understanding existing work on a particular subject. Moreover, topic modeling facilitates genre and style analysis, helping scholars discern distinguishing features of different literary genres or track the evolving styles of specific authors. It aids in character and plot analysis, unraveling the thematic intricacies within novels or plays. Comparative literature studies benefit from topic modeling by identifying common themes across different cultures and languages, fostering cross-cultural literary analysis. Additionally, the technique is instrumental in recognizing literary movements, mapping the evolution of trends within the field.\n",
    "\n",
    "The most common method for topic modeling is Latent Dirichlet Allocation (LDA). In the following section, we are going to show you how to run a simple LDA on a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts = lemmatization(documents)\n",
    "\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n",
      "act\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "corpus = []\n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "print(corpus[0][0:20])\n",
    "\n",
    "word = id2word[[0][:1][0]]\n",
    "print(word)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=8,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"say\" + 0.021*\"people\" + 0.011*\"other\" + 0.010*\"believe\" + 0.008*\"human\" + 0.008*\"word\" + 0.007*\"evidence\" + 0.007*\"true\" + 0.006*\"world\" + 0.006*\"life\"'),\n",
       " (1,\n",
       "  '0.704*\"ax\" + 0.049*\"max\" + 0.008*\"di\" + 0.007*\"pl\" + 0.004*\"tm\" + 0.004*\"wm\" + 0.003*\"ei\" + 0.003*\"bxn\" + 0.003*\"okz\" + 0.003*\"tq\"'),\n",
       " (2,\n",
       "  '0.033*\"key\" + 0.022*\"use\" + 0.021*\"government\" + 0.019*\"law\" + 0.012*\"system\" + 0.012*\"public\" + 0.011*\"action\" + 0.010*\"judge\" + 0.008*\"secret\" + 0.008*\"drug\"'),\n",
       " (3,\n",
       "  '0.010*\"issue\" + 0.009*\"group\" + 0.008*\"value\" + 0.008*\"report\" + 0.007*\"state\" + 0.007*\"provide\" + 0.007*\"rule\" + 0.006*\"new\" + 0.006*\"space\" + 0.005*\"israeli\"'),\n",
       " (4,\n",
       "  '0.024*\"drive\" + 0.019*\"use\" + 0.017*\"car\" + 0.014*\"buy\" + 0.014*\"system\" + 0.013*\"card\" + 0.012*\"price\" + 0.012*\"disk\" + 0.012*\"new\" + 0.011*\"sell\"'),\n",
       " (5,\n",
       "  '0.015*\"get\" + 0.013*\"know\" + 0.012*\"just\" + 0.011*\"make\" + 0.010*\"go\" + 0.010*\"so\" + 0.010*\"time\" + 0.010*\"think\" + 0.010*\"more\" + 0.010*\"good\"'),\n",
       " (6,\n",
       "  '0.033*\"use\" + 0.023*\"file\" + 0.017*\"program\" + 0.012*\"entry\" + 0.011*\"available\" + 0.010*\"include\" + 0.008*\"write\" + 0.008*\"window\" + 0.008*\"system\" + 0.008*\"thank\"'),\n",
       " (7,\n",
       "  '0.038*\"disease\" + 0.029*\"gun\" + 0.016*\"tobacco\" + 0.012*\"weapon\" + 0.011*\"condition\" + 0.010*\"criminal\" + 0.010*\"police\" + 0.010*\"crime\" + 0.010*\"arm\" + 0.009*\"control\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=8, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does LDA provides coherent topics? What insights do the results provide regarding the corpus?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
